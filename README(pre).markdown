# Persona-Driven Document Intelligence Engine

## Project Overview
This repository contains the complete source code and documentation for a high-performance system designed for the **Adobe India Hackathon 2025, Round 1B: Persona-Driven Document Intelligence**. The solution implements an end-to-end pipeline that ingests a collection of PDF documents, analyzes them based on a specified user persona and job-to-be-done, and extracts a ranked list of the most relevant sections.

The system is architected to run efficiently in a resource-constrained, offline, CPU-only Docker environment, adhering to all competition requirements.

## Table of Contents
- [System Architecture](#system-architecture)
- [Technical Implementation Details](#technical-implementation-details)
  - [Ingestion and Structuring](#ingestion-and-structuring)
  - [Query Decomposition](#query-decomposition)
  - [Hybrid Retrieval Engine](#hybrid-retrieval-engine)
  - [Re-ranking and Sub-Section Analysis](#re-ranking-and-sub-section-analysis)
- [Model Selection and Optimization](#model-selection-and-optimization)
- [Usage Instructions](#usage-instructions)
- [Directory Structure](#directory-structure)
- [Build and Run](#build-and-run)
- [Project Files](#project-files)

## System Architecture
The solution is implemented as a sequential, multi-stage pipeline designed to maximize both recall and precision, directly addressing the hackathon's scoring criteria.

*(Diagram illustrating the flow from PDF Input -> Ingestion -> Indexing -> Query -> Hybrid Retrieval -> Re-ranking -> JSON Output)*

The architectural flow is as follows:

1. **Ingestion and Structuring**: Each PDF in the input collection is parsed to extract raw text and identify structural elements (headings, text blocks). This output is segmented into semantically coherent, page-aware chunks.
2. **Indexing**: The processed text chunks are indexed in two parallel systems: a lexical index using the BM25Okapi algorithm and a semantic vector index using embeddings generated by the core neural model.
3. **Query Decomposition**: The input `request.json` (containing the persona and job-to-be-done) is deconstructed into a set of distinct, specific sub-queries to improve search focus.
4. **Stage 1: Hybrid Retrieval**: A broad search is executed across the document collection. The lexical and semantic indexes are queried in parallel, and the results are fused using a weighted scoring mechanism to produce a candidate list of promising document sections.
5. **Stage 2: Re-ranking and Analysis**: The candidate list from Stage 1 is subjected to a more focused, precision-oriented analysis. This stage re-ranks the candidates to determine the final `importance_rank` and performs a granular, sentence-level analysis within each section to generate the `refined_text`.
6. **Output Generation**: The final ranked and analyzed results are formatted into the required `output.json` structure, including all specified metadata.

## Technical Implementation Details

### Ingestion and Structuring
This module is responsible for converting raw PDF files into a structured, searchable format.

- **PDF Parsing**: The `PyMuPDF` (`fitz`) library is used for its high performance and detailed text extraction capabilities, including font information, boldness, and bounding box coordinates for every text span.
- **Structural Analysis**: A dynamic heading detection logic is employed. Instead of relying on static font sizes, the module first analyzes the font distribution across the entire document to establish a baseline for body text. Headings (H1, H2, H3) are then identified based on relative font size, bold weight, and positional information (e.g., centered text for titles).
- **Page-Aware Chunking**: The core innovation is the creation of granular, page-aware chunks. The logic from Round 1A was re-architected to segment content based not just on headings, but on the individual text blocks on each page under that heading. This ensures that every resulting chunk has an accurate page number, which is critical for the final output's citation accuracy.

### Query Decomposition
To effectively handle the complex user intent, the `job_to_be_done` string is parsed into multiple sub-queries. This is achieved with a rule-based approach using Python's `re` module, splitting the task description by common delimiters such as commas and conjunctions (e.g., "and"). Each resulting sub-task is then combined with the persona's role to form a set of focused queries.

### Hybrid Retrieval Engine
This module combines two distinct search paradigms to ensure comprehensive retrieval.

- **Lexical Search**: Implemented using the `rank_bm25` library. This component creates an inverted index of the document chunks and scores them based on the Okapi BM25 algorithm, which excels at matching exact keywords and domain-specific terms.
- **Semantic Search**: Implemented using the `sentence-transformers` library. It generates dense vector embeddings for each document chunk. Retrieval is performed by calculating the cosine similarity between the query embeddings and the document chunk embeddings.
- **Score Fusion**: The scores from both search systems are normalized to a common scale (0-1) and then combined using a weighted linear formula:
  ```
  final_score = (α * normalized_semantic_score) + ((1 - α) * normalized_bm25_score)
  ```
  The hyperparameter `α` (alpha) is set to 0.7, giving more weight to semantic relevance while still benefiting from keyword matching.

### Re-ranking and Sub-Section Analysis
This final stage refines the results from the hybrid retriever.

- **Sentence Segmentation**: For each top-ranked chunk, the content is first split into individual sentences using `nltk.sent_tokenize`.
- **Sentence-Level Scoring**: Each sentence is then embedded using the same core model. Its relevance is determined by calculating the maximum cosine similarity against the set of decomposed query vectors.
- **Refined Text Generation**: The top 2-3 highest-scoring sentences are concatenated to form the `refined_text` for the output. This provides a concise, highly relevant summary of the larger section.
- **Final Ranking**: The `importance_rank` is determined by re-sorting the candidate chunks based on the maximum sentence similarity score found within them.

## Model Selection and Optimization
The performance of the system is critically dependent on the choice and optimization of the embedding model.

- **Model Selection**: The `Alibaba-NLP/gte-large-en-v1.5` model was selected as the base model. The rationale for this choice is its optimal balance of high retrieval performance on the MTEB leaderboard, a manageable initial size (~434MB), and a large 8192-token context window, which is advantageous for processing long document sections.
- **Inference Engine**: ONNX Runtime is used as the execution backend. It provides a production-grade, cross-platform accelerator that is highly integrated with the Hugging Face ecosystem via the `optimum` library.
- **Quantization**: To meet the strict size (<1GB) and latency (<60s) constraints, Post-Training Static Quantization (PTQ) was applied. The FP32 model was converted to an INT8 model using the `optimum` library. This process reduces the on-disk model size to approximately 110MB and provides a significant (2x-4x) inference speedup on CPU by leveraging integer arithmetic. The quantization process uses a small calibration dataset to pre-calculate activation scales, preserving accuracy more effectively than dynamic quantization.

## Usage Instructions
The solution is fully containerized with Docker for easy and reproducible execution.

## Directory Structure
Ensure your project directory is structured as follows before running the Docker commands:

```
.
├── input/
│   ├── sample_acme_corp_report_2023.pdf
│   ├── sample_beta_inc_report_2023.pdf
│   └── request.json
├── output/
├── model/
│   └── model.onnx
├── Dockerfile
├── main.py
└── requirements.txt
```

## Build and Run

### Build the Docker Image
From the root directory, execute the following command:

```bash
docker build --platform linux/amd64 -t persona-solver:latest .
```

### Run the Container
Execute the solution using the `docker run` command. This command mounts the local `input` and `output` directories and runs the main script in an isolated environment:

```bash
docker run --rm \
  -v "$(pwd)/input:/app/input" \
  -v "$(pwd)/output:/app/output" \
  --network none \
  persona-solver:latest
```

The script will process all PDFs in the `input` directory and generate a single `output.json` file in the `output` directory.

## Project Files
- `main.py`: The main Python script containing the end-to-end application logic.
- `Dockerfile`: Defines the Docker image, dependencies, and execution environment.
- `requirements.txt`: Lists all required Python packages.
- `model/`: This directory contains the pre-optimized `model.onnx` file and any associated configuration files required by ONNX Runtime.
