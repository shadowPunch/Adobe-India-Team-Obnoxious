## Project Overview
This repository contains the complete source code and documentation for a high-performance system designed for the **Adobe India Hackathon 2025, Round 1B: Persona-Driven Document Intelligence**. The solution implements an end-to-end pipeline that ingests a collection of PDF documents, analyzes them based on a specified user persona and job-to-be-done, and extracts a ranked list of the most relevant sections.

The system is architected to run efficiently in a resource-constrained, offline, CPU-only Docker environment, adhering to all competition requirements. Additionally, the repository includes a separate solution for Round 1A, which addresses a related but independent part of the problem, housed in its own directory with a similar structure.

![graphviz (1)](https://github.com/user-attachments/assets/f63a41b1-af7c-487f-b0d2-56823acbbe7f)# Document Distillation pipeline for resource constrained Environment


## Table of Contents
- [System Architecture](#system-architecture)
- [Technical Implementation Details](#technical-implementation-details)
  - [Ingestion and Structuring](#ingestion-and-structuring)
  - [Query Decomposition](#query-decomposition)
  - [Hybrid Retrieval Engine](#hybrid-retrieval-engine)
  - [Re-ranking and Sub-Section Analysis](#re-ranking-and-sub-section-analysis)
- [Model Selection and Optimization](#model-selection-and-optimization)
- [Usage Instructions](#usage-instructions)
- [Directory Structure](#directory-structure)
- [Build and Run](#build-and-run)
- [Project Files](#project-files)

## System Architecture
The solution is implemented as a sequential, multi-stage pipeline designed to maximize both recall and precision, directly addressing the hackathon's scoring criteria.

The architectural flow is as follows:

1. **Ingestion and Structuring**: Each PDF in the input collection is parsed to extract raw text and identify structural elements (headings, text blocks). This output is segmented into semantically coherent, page-aware chunks.
2. **Indexing**: The processed text chunks are indexed in two parallel systems: a lexical index using the BM25Okapi algorithm and a semantic vector index using embeddings generated by the core neural model.
3. **Query Decomposition**: The input JSON file (containing the persona and job-to-be-done) is deconstructed into a set of distinct, specific sub-queries to improve search focus.
4. **Stage 1: Hybrid Retrieval**: A broad search is executed across the document collection. The lexical and semantic indexes are queried in parallel, and the results are fused using a weighted scoring mechanism to produce a candidate list of promising document sections.
5. **Stage 2: Re-ranking and Analysis**: The candidate list from Stage 1 is subjected to a more focused, precision-oriented analysis. This stage re-ranks the candidates to determine the final importance rank and performs a granular, sentence-level analysis within each section to generate the refined_text.
6. **Output Generation**: The final ranked and analyzed results are formatted into the required output json structure, including all specified metadata.

## Technical Implementation Details

### Ingestion and Structuring
This module is responsible for converting raw PDF files into a structured, searchable format.

- **PDF Parsing**: The PyMuPDF library is used for its high performance and detailed text extraction capabilities, including font information, boldness, and bounding box coordinates for every text span.
- **Structural Analysis**: A dynamic heading detection logic is employed. Instead of relying on static font sizes, the module first analyzes the font distribution across the entire document to establish a baseline for body text. Headings (H1, H2, H3) are then identified based on relative font size, bold weight, and positional information (e.g., centered text for titles).
- **Page-Aware Chunking**: Creation of granular, page-aware chunks on basis of which content is segmented based not just on headings, but on the individual text blocks on each page under that heading. This ensures that every resulting chunk has an accurate page number, which is critical for the final output's citation accuracy.

### Query Decomposition
To effectively handle the complex user intent, the user specified job string is parsed into multiple sub-queries. This is achieved with a rule-based approach, splitting the task description by common delimiters such as commas and conjunction. Each resulting sub-task is then combined with the persona's role to form a set of focused queries.

### Hybrid Retrieval Engine
This module combines two distinct search paradigms to ensure comprehensive retrieval.

- **Lexical Search**: Implemented using the `rank_bm25` library. This component creates an inverted index of the document chunks and scores them based on the Okapi BM25 algorithm, used for matching exact keywords and domain-specific terms.
- **Semantic Search**: Implemented using the sentence-transformer library. It generates dense vector embeddings for each document chunk. Retrieval is performed by calculating the cosine similarity between the query embeddings and the document chunk embeddings.
- **Score Fusion**: The scores from both search systems are normalized to a common scale (0-1) and then combined using a weighted linear formula:
  ```
  final_score = (α * normalized_semantic_score) + ((1 - α) * normalized_bm25_score)
  ```
  
### Re-ranking and Sub-Section Analysis
This final stage refines the results from the hybrid retriever.

- **Sentence Segmentation**: For each top-ranked chunk, the content is first split into individual sentences.
- **Sentence-Level Scoring**: Each sentence is then embedded using the same core model. Its relevance is determined by calculating the maximum cosine similarity against the set of decomposed query vectors.
- **Refined Text Generation**: The top 2-3 highest-scoring sentences are concatenated to form the refined text for the output. This provides a concise, highly relevant summary of the larger section.
- **Final Ranking**: The importance rank is determined by re-sorting the candidate chunks based on the maximum sentence similarity score found within them.

## Model Selection and Optimization
The performance of the system is critically dependent on the choice and optimization of the embedding model.

- **Model Selection**: The `Alibaba-NLP/gte-large-en-v1.5` model was selected as the base model. The rationale for this choice is its optimal balance of high retrieval performance on the MTEB leaderboard, a manageable initial size (~434MB), and a large 8192-token context window, which is advantageous for processing long document sections.
- **Inference Engine**: ONNX Runtime is used as the execution backend.
- **Quantization**: Post-Training Static Quantization (PTQ) was applied. The FP32 model was converted to INT8. This process reduces the on-disk model size to approximately 110MB and provides a significant (4x) inference speedup on CPU. The quantization process uses calibration dataset to pre-calculate activation scales, preserving accuracy more effectively than dynamic quantization.

## Usage Instructions
The solution is fully containerized with Docker for easy and reproducible execution. Ensure your project directory is structured as follows before running the Docker commands. The repository contains two main folders: round1a and round1b, each with its own independent solution and Docker environment.

```
.
├── round1a/
│   ├── input/
│   │   ├── 51.pdf
│   │   └── request.json
│   ├── output/
│   ├── model/
│   │   └── model.onnx
│   ├── Dockerfile
│   ├── main.py
│   └── requirements.txt
├── round1b/
│   ├── input/
│   │   ├── 51.pdf
│   │   └── request.json
│   ├── output/
│   ├── model/
│   │   └── model.onnx
│   ├── Dockerfile
│   ├── main.py
│   └── requirements.txt
└── README.md
```

- **`round1a/`**: Contains the solution for Round 1A, including its own `Dockerfile`, `main.py`, `requirements.txt`, and dedicated `input/`, `output/`, and `model/` directories.
- **`round1b/`**: Contains the solution for Round 1B (Persona-Driven Document Intelligence), with a similar structure to `round1a/` but with its own independent implementation and `Dockerfile`.

## Build and Run

Each round's solution can be built and run independently using its respective `Dockerfile`.

### Round 1A

```bash
cd round1a
```

**Build the Docker Image**:
```bash
docker build --platform linux/amd64 -t structurer-round1a:latest .
```

**Run the Container**:
```bash
docker run --rm \
  -v "$(pwd)/input:/app/input" \
  -v "$(pwd)/output:/app/output" \
  --network none \
  structurer-round1a:latest
```

### Round 1B

```bash
cd round1b
```

**Build the Docker Image**:
```bash
docker build --platform linux/amd64 -t persona-solver-round1b:latest .
```

**Run the Container**:
```bash
docker run --rm \
  -v "$(pwd)/input:/app/input" \
  -v "$(pwd)/output:/app/output" \
  --network none \
  persona-solver-round1b:latest
```

For both rounds, the script will process all PDFs in the respective `input/` directory and generate a single `output.json` file in the corresponding `output/` directory.

## Project Files
Each round (`round1a/` and `round1b/`) contains the following files:

- `main.py`: The main Python script containing the end-to-end application logic for the respective round.
- `Dockerfile`: Defines the Docker image, dependencies, and execution environment for the respective round.
- `requirements.txt`: Lists all required Python packages for the respective round.
- `model/` : Contains the pre-optimized `model.onnx` file and any associated configuration files required by ONNX Runtime for the respective round.
